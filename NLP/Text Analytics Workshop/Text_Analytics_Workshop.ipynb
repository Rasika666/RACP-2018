{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Analytics Workshop - Research & Academic Collaboration Program, University of Kelaniya\n",
    "#### 5th July 2018\n",
    "\n",
    "###### Ruvan Weerasinghe \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The libraries/packages we need\n",
    "\n",
    "- Pandas is for converting/reading Data Frames\n",
    "- Numpy is a very useful math library\n",
    "- Normalization and Utils are two helper programs we have defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from normalization import normalize_corpus\n",
    "from utils import build_feature_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to ensure that Pandas is installed (automatic in Anaconda - else use 'pip' to install)\n",
    "Need to ensure that our two Python scripts normalization.py and utils.py are in the Python path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data file as csv using Pandas\n",
    "\n",
    "This is done using the read_csv() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n"
     ]
    }
   ],
   "source": [
    "# Load the cleaned movie reviews dataset\n",
    "dataset = pd.read_csv(r'movie_reviews.csv')\n",
    "# Check how big the dataset frame is using len() function\n",
    "# Print the first few data points - note that data consists of 2 columns named 'review' and 'sentiment'\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should show the first 5 reviews together with their sentiment labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the training and testing sets\n",
    "\n",
    "- We first divide the dataset to training and test sets\n",
    "- Then we use 4 arrays to store the 'review' and 'sentiment' parts of each set separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Divide data into training and testing sets\n",
    "train_data = dataset[:25000]\n",
    "test_data = dataset[25000:] \n",
    "# Check size (len) and first few elements (head()) of test_data (sub)frame\n",
    "\n",
    "\n",
    "# Divide the data into the data (review) and the label (sentiment) in both training and testing sets\n",
    "train_reviews = np.array(train_data['review'])\n",
    "train_sentiments = np.array(train_data['sentiment'])\n",
    "test_reviews = np.array(test_data['review'])\n",
    "test_sentiments = np.array(test_data['sentiment'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try examining the length of each array and some elements within it to see if it is what you expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We clean/normalize/wrangle the input reviews the way we want\n",
    "\n",
    "- Here we simply say we don't need to lemmatize the words (default is to lemmatize)\n",
    "- And that we are only interested in text characters (so we loose terms such as '007')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalize the training review data using the normalization.py module\n",
    "norm_train_reviews = normalize_corpus(train_reviews,\n",
    "                                      lemmatize=False,\n",
    "                                      only_text_chars=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process would take a few minutes - see the code in normalization.py to understand why"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can now extract the features that we are interested in\n",
    "\n",
    "- We extract tfidf weights instead of simply counts (frequency)\n",
    "- We also stick to unigrams (i.e. individual words) and not bigrams or trigrams\n",
    "- We want to consider all words - even those that occur only once (possibly missplet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract features from these normalized training reviews\n",
    "# - which features? Try other features using parameters provided in utils.py                                                                           \n",
    "vectorizer, train_features = build_feature_matrix(documents=norm_train_reviews,\n",
    "                                                  feature_type='tfidf',\n",
    "                                                  ngram_range=(1, 1), \n",
    "                                                  min_df=0.0, max_df=1.0)                                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is vectorizer and what is train_features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train an SVM model using the training data\n",
    "\n",
    "We call scikit-learn's SGDClassifier class for this\n",
    "NB: scikit-learn has many other Machine Learning algorithms you can try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', n_iter=500, n_jobs=1,\n",
       "       penalty='l2', power_t=0.5, random_state=None, shuffle=True,\n",
       "       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Build/train an SVM classifier model with the train features extracted from reviews\n",
    "svm = SGDClassifier(loss='hinge', n_iter=500)\n",
    "svm.fit(train_features, train_sentiments) # We give the features and the correct labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test how good our model is\n",
    "\n",
    "In order to test how good our model is, we need to also transform our test set the same way as the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalize the test reviews                        \n",
    "norm_test_reviews = normalize_corpus(test_reviews,\n",
    "                                     lemmatize=False,\n",
    "                                     only_text_chars=True)  \n",
    "# Extract features from the normalized test reviews                                   \n",
    "test_features = vectorizer.transform(norm_test_reviews)         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We finally test the output of the trained model on the test data set\n",
    "\n",
    "- We first send our vectorized test reviews to the model to get the predictions\n",
    "- Then we use 3 functions we have defined in our utils.py package to output the performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.89\n",
      "Precision: 0.88\n",
      "Recall: 0.91\n",
      "F1 Score: 0.9\n",
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive       3712      359\n",
      "        negative        501     3543\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   positive       0.88      0.91      0.90      4071\n",
      "   negative       0.91      0.88      0.89      4044\n",
      "\n",
      "avg / total       0.89      0.89      0.89      8115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict the sentiment for test dataset movie reviews\n",
    "predicted_sentiments = svm.predict(test_features)       \n",
    "\n",
    "# Evaluate model prediction performance by comparing predicted sentiments and test sentiments\n",
    "from utils import display_evaluation_metrics, display_confusion_matrix, display_classification_report\n",
    "\n",
    "# Show performance metrics\n",
    "display_evaluation_metrics(true_labels=test_sentiments,\n",
    "                           predicted_labels=predicted_sentiments,\n",
    "                           positive_class='positive')  \n",
    "\n",
    "# Show confusion matrix\n",
    "display_confusion_matrix(true_labels=test_sentiments,\n",
    "                         predicted_labels=predicted_sentiments,\n",
    "                         classes=['positive', 'negative'])\n",
    "\n",
    "# Show detailed per-class classification report\n",
    "display_classification_report(true_labels=test_sentiments,\n",
    "                              predicted_labels=predicted_sentiments,\n",
    "                              classes=['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many things you can try:\n",
    "(a) change the way you 'clean' the data (e.g. remove terms that occur less than a minimum number of times?\n",
    "(b) change the kind of features you extract (e.g. counts instead of tfidf weights? bigrams and trigrams?\n",
    "(c) change the learning algorithm from SVM to another supervised algorithm (e.g. Logistic Regression, Naive Bayes, Decision Tree?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
